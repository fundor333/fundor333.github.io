{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec2a032c",
   "metadata": {},
   "source": [
    "---\n",
    "date: '2025-09-03T17:13:02+08:00'\n",
    "title: 'Generate Dataframe Summaries With Python'\n",
    "feature_link: \"https://www.midjourney.com/home/\"\n",
    "feature_text: \"by AI Midjourney\"\n",
    "description: 'How to generate dataframe summaries with python and AI for a type of dataset'\n",
    "isStarred: false\n",
    "tags:\n",
    "- datascience\n",
    "- dataframe\n",
    "- pandas\n",
    "- llm\n",
    "- Ollama\n",
    "- mistral\n",
    "categories:\n",
    "- dev\n",
    "images:\n",
    "keywords:\n",
    "series:\n",
    "- Data and Data Tools\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1df9197-c3c4-48d0-b985-90fc43db7f5b",
   "metadata": {},
   "source": [
    "How much time do you spend with making summaries of dataset? Too much and I don't like doing it so I search to do it with the AI. So this is my sperimentation with some medical data see at PyDataVe 22nd event and Mistral model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569e2513-b9d5-4d73-94b0-e9c72fb44152",
   "metadata": {},
   "source": [
    "## The code for the inizializzation\n",
    "\n",
    "For start I need to install some dipendency\n",
    "\n",
    "~~~ text\n",
    "langchain>=0.3.27\n",
    "langchain-ollama>=0.3.7\n",
    "pandas>=2.3.2\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67c897ff-5971-4392-ab35-0cbad5600d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "Dataset shape: (418, 20)\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "Missing value stats:\n",
      "ID                 0\n",
      "N_Days             0\n",
      "Status             0\n",
      "Drug             106\n",
      "Age                0\n",
      "Sex                0\n",
      "Ascites          106\n",
      "Hepatomegaly     106\n",
      "Spiders          106\n",
      "Edema              0\n",
      "Bilirubin          0\n",
      "Cholesterol      134\n",
      "Albumin            0\n",
      "Copper           108\n",
      "Alk_Phos         106\n",
      "SGOT             106\n",
      "Tryglicerides    136\n",
      "Platelets         11\n",
      "Prothrombin        2\n",
      "Stage              6\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain_ollama import ChatOllama\n",
    "from typing import Literal\n",
    "\n",
    "df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "print(\"-*-\" * 20)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"-*-\" * 20)\n",
    "print(\"Missing value stats:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c1d5d9-33ce-4d19-82ab-800ec5ce3c45",
   "metadata": {},
   "source": [
    "This is a section of the dataset and what is missing value of the stats.\n",
    "\n",
    "Now we will start with the AI. In my case I user Ollama with Mistral model.\n",
    "I install the model with \n",
    "\n",
    "~~~ bash\n",
    "ollama run mistral\n",
    "~~~\n",
    "\n",
    "And prepare the code for use the model. First you need to make a connection with the local LLM instance. This code use Mistral but you can pass any local LLM instance you have.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c32630b6-bc6c-40b1-a7a3-fabd7dcffa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm(model_name: str = \"mistral:latest\") -> ChatOllama:\n",
    "    \"\"\"\n",
    "    Create and configure a ChatOllama instance for local LLM inference.\n",
    "    \n",
    "    This function initializes a ChatOllama client configured to connect to a\n",
    "    local Ollama server. The client is set up with deterministic output\n",
    "    (temperature=0) for consistent responses across multiple calls with the\n",
    "    same input.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : str, optional\n",
    "        The name of the Ollama model to use for chat completions.\n",
    "        Must be a valid model name that is available on the local Ollama\n",
    "        installation. Default is \"mistral:latest\".\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ChatOllama\n",
    "        A configured ChatOllama instance ready for chat completions.\n",
    "    \"\"\"\n",
    "    return ChatOllama(\n",
    "        model=model_name, base_url=\"http://localhost:11434\", temperature=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d5654b-831e-4cf4-a634-73b2dcf00e29",
   "metadata": {},
   "source": [
    "If you want to test the connection you can use this command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cb7338-77e1-47d1-9b5f-6126b2cede4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hallo! Wie geht es Ihnen? Ich bin hier, um Ihnen zu helfen. Was möchten Sie heute tun?\n",
      "\n",
      "Ich kann Ihnen beispielsweise helfen:\n",
      "\n",
      "* Fragen beantworten\n",
      "* Informationen suchen\n",
      "* Aufgaben lösen\n",
      "* und vieles mehr!\n",
      "\n",
      "Welche Aufgabe haben wir heute vor uns?\n"
     ]
    }
   ],
   "source": [
    "print(get_llm().invoke(\"test\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ec2c60-0f16-4b6c-a763-c7d64ea9b6a2",
   "metadata": {},
   "source": [
    "## Make a context\n",
    "\n",
    "Now we need to generate a context for the LLM. If you do this function with all the necessary data you can relaunch this script every time you need a new README/summary of the dataset. This is better to be a dataset with a fixed schema and a date which change every year like medical data (this), monthly sell report, census data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73b658eb-d006-46fa-9b0d-a498cb6b5d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_context_message(df: pd.DataFrame, dataset_name:str) -> str:\n",
    "    # Basic application statistics\n",
    "    total_analisys = len(df)\n",
    "\n",
    "    # Gender distribution\n",
    "    gender_counts = df[\"Sex\"].value_counts()\n",
    "    male_count = gender_counts.get(\"M\", 0)\n",
    "    female_count = gender_counts.get(\"F\", 0)\n",
    "\n",
    "    # Stage Statistics\n",
    "    stage_data = df[\"Stage\"].dropna()\n",
    "    stage_avg = stage_data.mean()\n",
    "    stage_25th = stage_data.quantile(0.25)\n",
    "    stage_50th = stage_data.quantile(0.50)\n",
    "    stage_75th = stage_data.quantile(0.75)\n",
    "\n",
    "    # NDays Statistics\n",
    "    days_data = df[\"N_Days\"].dropna()\n",
    "    days_avg = days_data.mean()\n",
    "    days_25th = days_data.quantile(0.25)\n",
    "    days_50th = days_data.quantile(0.50)\n",
    "    days_75th = days_data.quantile(0.75)\n",
    "\n",
    "    def status_category(exp):\n",
    "        if pd.isna(exp):\n",
    "            return \"Unkown\"\n",
    "        elif exp == \"C\":\n",
    "            return \"Censored\"\n",
    "        elif exp == \"CL\":\n",
    "            return \"Censored due to Lever tx\"\n",
    "        elif exp == \"D\":\n",
    "            return \"Death\"\n",
    "        else:\n",
    "            return \"Unkow\"\n",
    "\n",
    "    df['Status Str']= df['Status'].apply(status_category)\n",
    "    status_str_stats = []\n",
    "\n",
    "    for category in [\"Censored\", \"Censored due to Lever tx\", \"Death\",]: \n",
    "        category_data = df[df[\"Status Str\"] == category]\n",
    "        if len(category_data) > 0:\n",
    "            male = len(category_data[category_data[\"Sex\"] == \"M\"])\n",
    "            female = len(category_data[category_data[\"Sex\"] == \"F\"])\n",
    "            total = len(category_data)\n",
    "            rate_m = (male / total) * 100\n",
    "            rate_f = (female / total) * 100\n",
    "            status_str_stats.append((category, male, female, total, rate_m, rate_f))\n",
    "\n",
    "    summary =f\"\"\"{dataset_name}\n",
    "    \n",
    "Total Analisys: {total_analisys:,}\n",
    "\n",
    "Gender Distribution:\n",
    "- Male applicants: {male_count:,} ({male_count/total_analisys*100:.1f}%)\n",
    "- Female applicants: {female_count:,} ({female_count/total_analisys*100:.1f}%)\n",
    "\n",
    "Stage Statistics:\n",
    "- Average Stage: {stage_avg:.2f}\n",
    "- 25th percentile: {stage_25th:.2f}\n",
    "- 50th percentile (median): {stage_50th:.2f}\n",
    "- 75th percentile: {stage_75th:.2f}\n",
    "\n",
    "N Day Statistics:\n",
    "- N Days Stage: {days_avg:.2f}\n",
    "- 25th percentile: {days_25th:.2f}\n",
    "- 50th percentile (median): {days_50th:.2f}\n",
    "- 75th percentile: {days_75th:.2f}\n",
    "\"\"\"\n",
    "\n",
    "    summary += \"\\n\\nStatus Rates by Sex:\"\n",
    "    for category, male, female, total, rate_m, rate_f in status_str_stats:\n",
    "        summary += (\n",
    "            f\"\\n- {category}: {male}/{total} Male ({rate_m:.1f}% rate)\"+\n",
    "            f\"\\n- {category}: {female}/{total} Female ({rate_f:.1f}% rate)\"\n",
    "\n",
    "        )\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851f7e68-d016-4c7c-8cce-bcd4374f6870",
   "metadata": {},
   "source": [
    "## Make a report\n",
    "\n",
    "After checking all you need to have a template for the repo of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f6b679b-f3de-4d4c-8167-0934aca4eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARIZE_DATAFRAME_PROMPT = \"\"\"\n",
    "You are an expert data analyst and data summarizer. \n",
    "Your task is to take in complex datasets and return user-friendly descriptions and findings.\n",
    "\n",
    "You were given this dataset:\n",
    "- Name: {dataset_name}\n",
    "- Source: {dataset_source}\n",
    "\n",
    "This dataset was analyzed in a pipeline before it was given to you.\n",
    "These are the findings returned by the analysis pipeline:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Based on these findings, write a detailed report in {report_format} format.\n",
    "Give the report a meaningful title and separate findings into sections with headings and subheadings.\n",
    "Output only the report in {report_format} and nothing else.\n",
    "\n",
    "Report:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5f4a0d-4cae-49ab-add2-901c77100869",
   "metadata": {},
   "source": [
    "This prompt and a lot of the code of this article are from [this post](https://towardsdatascience.com/llms-pandas-how-i-use-generative-ai-to-generate-pandas-dataframe-summaries-2/).\n",
    "\n",
    "After this we need a function that take the dataset *df*, the prompt *SUMMARIZE_DATAFRAME_PROMPT* with the needed info and return the content of the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f27c65b-31ed-4b87-b4fd-8d57d6021e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_report_summary(\n",
    "    dataset: pd.DataFrame,\n",
    "    dataset_name: str,\n",
    "    dataset_source: str,\n",
    "    report_format: Literal[\"markdown\", \"html\"] = \"markdown\",\n",
    ") -> str:\n",
    "    context_message = get_summary_context_message(df=dataset, dataset_name=dataset_name)\n",
    "    prompt = SUMMARIZE_DATAFRAME_PROMPT.format(\n",
    "        dataset_name=dataset_name,\n",
    "        dataset_source=dataset_source,\n",
    "        context=context_message,\n",
    "        report_format=report_format,\n",
    "    )\n",
    "    return get_llm().invoke(input=prompt).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2374dccc-01d9-4dc8-b591-ede7ae5b419d",
   "metadata": {},
   "source": [
    "In our case we launch it as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "677373bc-1e43-4d2c-b07a-02f0e1d0a761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # Cirrhosis Patient Survival Prediction Analysis Report\n",
      "\n",
      "## Overview\n",
      "The dataset analyzed consists of 418 records related to cirrhosis patients, sourced from [Kaggle](https://www.kaggle.com/datasets/joebeachcapital/cirrhosis-patient-survival-prediction/data). The data provides information about the patient's gender, stage of cirrhosis, number of days since diagnosis, and final status (censored or death).\n",
      "\n",
      "## Demographics\n",
      "### Gender Distribution\n",
      "The dataset shows a significant imbalance in gender distribution with 89.5% female applicants (374) and only 10.5% male applicants (44).\n",
      "\n",
      "## Cirrhosis Stage Statistics\n",
      "### Average Stage\n",
      "The average stage of cirrhosis for the analyzed patients is 3.02, indicating a severe level of liver damage.\n",
      "\n",
      "### Percentiles\n",
      "- **25th percentile**: The cirrhosis stage is at least 2.00 for 25% of the patients.\n",
      "- **Median (50th percentile)**: Half of the patients have a cirrhosis stage of 3.00.\n",
      "- **75th percentile**: For 75% of the patients, the cirrhosis stage is 4.00 or lower.\n",
      "\n",
      "## N Days Statistics\n",
      "### N Days Stage\n",
      "The average number of days since diagnosis for the analyzed patients is 1917.78 days.\n",
      "\n",
      "### Percentiles\n",
      "- **25th percentile**: The minimum number of days since diagnosis for 25% of the patients is 1092.75 days.\n",
      "- **Median (50th percentile)**: Half of the patients have been diagnosed with cirrhosis for at least 1730.00 days.\n",
      "- **75th percentile**: For 75% of the patients, the number of days since diagnosis is 2613.50 days or less.\n",
      "\n",
      "## Status Rates by Sex\n",
      "The following table shows the rates of different statuses (censored due to Lever tx and death) for both male and female applicants:\n",
      "\n",
      "|                     | Male Applicants | Female Applicants |\n",
      "|---------------------|-----------------|-------------------|\n",
      "| Censored            | 17/232 (7.3%)    | 215/232 (92.7%)   |\n",
      "| Censored due to Lever tx | 3/25 (12.0%)     | 22/25 (88.0%)     |\n",
      "| Death                | 24/161 (14.9%)   | 137/161 (85.1%)   |\n",
      "\n",
      "The analysis indicates that female applicants are more likely to have their status censored, either due to the lack of information or other factors, while male applicants are more likely to experience death. However, it's important to note that the sample size for male applicants is significantly smaller than that of female applicants.\n"
     ]
    }
   ],
   "source": [
    "md_report = get_report_summary(\n",
    "    dataset=df, \n",
    "    dataset_name=\"Cirrhosis Patient Survival Prediction\",\n",
    "    dataset_source=\"https://www.kaggle.com/datasets/joebeachcapital/cirrhosis-patient-survival-prediction/data\"\n",
    ")\n",
    "print(md_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
